{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    " # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print('----------START GPU----------')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "            print('----------END GPU----------')\n",
    "    except RuntimeError as e:\n",
    " # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import some packages\n",
    "\n",
    "You should load packages you may need in this project first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import skimage.io\n",
    "import skimage.color\n",
    "import skimage.transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras import backend as K\n",
    "\n",
    "#Sklearn libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load character data from the dataset subfolder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a list of paths for images\n",
    "imagepaths = []\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('1125data'):\n",
    "    for filename in filenames:\n",
    "        path = os.path.join(dirname, filename)\n",
    "        #print(os.path.join(dirname, filename))\n",
    "        if path.endswith(\"png\"):\n",
    "            imagepaths.append(path)\n",
    "            \n",
    "print(len(imagepaths))\n",
    "\n",
    "#print(imagepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Defining a function that plots the image selected from a path\n",
    "\n",
    "# def img_plot(img_path):\n",
    "#     img = cv2.imread(img_path)\n",
    "#     #convert to RGB space\n",
    "#     img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     image_grayscale_blurred = cv2.GaussianBlur(img_rgb, (15,15), 0)\n",
    "#     im3 =  cv2.resize(image_grayscale_blurred, (150,150), interpolation = cv2.INTER_AREA)  \n",
    "#  #ar = np.array(resized_img)\n",
    "#  #ar = resized_img.reshape(1,784) \n",
    "     \n",
    "# #check the shape of the image\n",
    "#     print(\"Shape of the image is \", im3.shape)\n",
    "#     print(img_rgb.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_plot(imagepaths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#skimage.io.imread()\n",
    "#skimage.color.rgb2gray()\n",
    "data_dir = './1125data/'\n",
    "classes = os.listdir(data_dir)\n",
    "data = []\n",
    "for cls in classes:\n",
    "    files = os.listdir(data_dir+cls)\n",
    "    for f in files:        \n",
    "        img = skimage.io.imread(data_dir+cls+\"/\"+f)\n",
    "#         img_rgb = skimage.color.rgb2gray(img)\n",
    "#         image_grayscale_blurred = cv2.GaussianBlur(img_rgb, (15,15), 0)\n",
    "        img =cv2.resize(img,(224, 224))\n",
    "\n",
    "        data.append({\n",
    "            'x':img,\n",
    "            'y':cls\n",
    "        })\n",
    "        \n",
    "random.shuffle(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create train data set and test data set \n",
    "Using 80/20 rule, 80 precent for trainning, and 20 percent for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train =\n",
    "#y_train =\n",
    "\n",
    "#x_test =\n",
    "#y_test =\n",
    "X = [d['x'] for d in data]\n",
    "y = [d['y'] for d in data]\n",
    "\n",
    "y_dic = list(np.unique(y))\n",
    "y = [y_dic.index(v) for v in y]\n",
    "x_train = np.array(X[:int(len(X)*0.8)])\n",
    "y_train = np.array(y[:int(len(X)*0.8)])\n",
    "\n",
    "x_test = np.array(X[int(len(X)*0.8):])\n",
    "y_test = np.array(y[int(len(X)*0.8):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examming and Understand data set for trainning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # you may print out some infomation about train data set \n",
    "# print(x_train[0])\n",
    "# # print(y_train)\n",
    "# print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decide a few parameters for trainning\n",
    "\n",
    "such as batch size, epochs, image size in rows and colomns, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32#batchsize：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；\n",
    "\n",
    "num_classes = len(classes)\n",
    "epochs =  10\n",
    "\n",
    "## input image dimensions\n",
    "img_rows, img_cols =224,224\n",
    "\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras.utils.to_categorical()\n",
    "def extend_channel(data):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        data = data.reshape(data.shape[0], 1, img_rows, img_cols)\n",
    "    else:\n",
    "        data = data.reshape(data.shape[0], img_rows, img_cols, 1)\n",
    "        \n",
    "    return data\n",
    "\n",
    "x_train = extend_channel(x_train)\n",
    "x_test = extend_channel(x_test)\n",
    "\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "y_train2 = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test2 = keras.utils.to_categorical(y_test, num_classes)\n",
    "input_shape\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# datagen =ImageDataGenerator(\n",
    "#         featurewise_center=False,  # set input mean to 0 over the dataset 布尔值，使输入数据集去中心化（均值为0）, 按feature执行\n",
    "#         samplewise_center=False,  # set each sample mean to 0 布尔值，使输入数据的每个样本均值为0 \n",
    "#         featurewise_std_normalization=False,  # divide inputs by std of the dataset布尔值，将输入除以数据集的标准差以完成标准化, 按feature执行。\n",
    "#         samplewise_std_normalization=False,  # divide each input by its std布尔值，将输入的每个样本除以其自身的标准差\n",
    "#         zca_whitening=False,  # apply ZCA whitening布尔值，对输入数据施加ZCA白化\n",
    "#         rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "#         zoom_range = 0.1, # Randomly zoom image \n",
    "#         width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "#         height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "#         horizontal_flip=False,  # randomly flip images\n",
    "#         vertical_flip=False)  # randomly flip images\n",
    "# datagen.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a CNN model for character recognition\n",
    "\n",
    "This is an important part of this project. You have to create a cnn model using tensorflow and keras to train the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.0001)\n",
    "# learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience = 2, verbose=1,factor=0.5, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alexnet_our version\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (224,224,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides = 2, padding = 'Same'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides = 2, padding = 'Same'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides = 2, padding = 'Same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10, activation = \"softmax\"))\n",
    "\n",
    "#AlexNet_original\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(96,(11,11),strides=(4,4),input_shape=(227,227,3),padding='valid',activation='relu',kernel_initializer='uniform'))\n",
    "# model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
    "# model.add(Conv2D(256,(5,5),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "# model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
    "# model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "# model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "# model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "# model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(4096,activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(4096,activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(1000,activation='softmax'))\n",
    "\n",
    "#Z_net\n",
    "# model = Sequential()  \n",
    "# model.add(Conv2D(96,(7,7),strides=(2,2),input_shape=(224,224,1),padding='valid',activation='relu',kernel_initializer='uniform'))  \n",
    "# model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "# model.add(Conv2D(256,(5,5),strides=(2,2),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "# model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "# model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "# model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "# model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "# model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "# model.add(Flatten())  \n",
    "# model.add(Dense(4096,activation='relu'))  \n",
    "# model.add(Dropout(0.5))  \n",
    "# model.add(Dense(4096,activation='relu'))  \n",
    "# model.add(Dropout(0.5))  \n",
    "# model.add(Dense(1000,activation='softmax')) \n",
    "\n",
    "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile()\n",
    "# #model.summary()\n",
    "\n",
    "import tensorflow as tf\n",
    "# 必须要下面这行代码\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "print(tf.__version__)\n",
    " \n",
    "# 我自己使用的函数\n",
    "def get_flops_params():\n",
    "    sess = tf.compat.v1.Session()\n",
    "    graph = sess.graph\n",
    "    flops = tf.compat.v1.profiler.profile(graph, options=tf.compat.v1.profiler.ProfileOptionBuilder.float_operation())\n",
    "    params = tf.compat.v1.profiler.profile(graph, options=tf.compat.v1.profiler.ProfileOptionBuilder.trainable_variables_parameter())\n",
    "    print('FLOPs: {};    Trainable params: {}'.format(flops.total_float_ops, params.total_parameters))\n",
    " \n",
    " \n",
    "# 网上推荐的\n",
    "# sess = tf.compat.v1.Session()\n",
    "# graph = sess.graph\n",
    "# stats_graph(graph)\n",
    "def stats_graph(graph):\n",
    "    flops = tf.compat.v1.profiler.profile(graph, options=tf.compat.v1.profiler.ProfileOptionBuilder.float_operation())\n",
    "    # print('FLOPs: {}'.format(flops.total_float_ops))\n",
    "    params = tf.compat.v1.profiler.profile(graph, options=tf.compat.v1.profiler.ProfileOptionBuilder.trainable_variables_parameter())\n",
    "    # print('Trainable params: {}'.format(params.total_parameters))\n",
    "    print('FLOPs: {};    Trainable params: {}'.format(flops.total_float_ops, params.total_parameters))\n",
    " \n",
    " \n",
    "def get_flops(model):\n",
    "    run_meta = tf.compat.v1.RunMetadata()\n",
    "    opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "    # We use the Keras session graph in the call to the profiler.\n",
    "    flops = tf.compat.v1.profiler.profile(graph=tf.compat.v1.keras.backend.get_session().graph, run_meta=run_meta, cmd='op', options=opts)\n",
    "    return flops.total_float_ops  # Prints the \"flops\" of the model.\n",
    " \n",
    "# 必须使用tensorflow中的keras才能够获取到FLOPs, 模型中的各个函数都必须使用tensorflow.keras中的函数，和keras混用会报错\n",
    "\n",
    "# 获取模型每一层的参数详情\n",
    "model.summary()\n",
    "# 获取模型浮点运算总次数和模型的总参数\n",
    "get_flops_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "def time_per_layer(model):\n",
    "    new_model = model\n",
    "\n",
    "    times = np.zeros((len(model.layers), 2))\n",
    "    inp = np.ones((224, 224, 1))\n",
    "\n",
    "    for i in range(1, len(model.layers)):\n",
    "        new_model = keras.models.Model(inputs=[model.input], outputs=[model.layers[-i].output])\n",
    "\n",
    "        # new_model.summary()\n",
    "        new_model.predict(inp[None, :, :, :])\n",
    "\n",
    "        t_s = time.time()\n",
    "        new_model.predict(inp[None, :, :, :])\n",
    "        t_e2 = time.time() - t_s\n",
    "\n",
    "        times[i, 1] = t_e2\n",
    "        del new_model\n",
    "\n",
    "    for i in range(0, len(model.layers) - 1):\n",
    "        times[i, 0] = abs(times[i + 1, 1] - times[i, 1])\n",
    "\n",
    "    times[-1, 0] = times[-1, 1]\n",
    "\n",
    "\n",
    "    return times\n",
    "\n",
    "# from net_flops import net_flops\n",
    "\n",
    "# net_flops(model,table=True)\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "times = time_per_layer(model)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "x = [model.layers[-i].name for i in range(1,len(model.layers))]\n",
    "#x = [i for i in range(1,len(model.layers))]\n",
    "g = [times[i,0] for i in range(1,len(times))]\n",
    "x_pos = np.arange(len(x))\n",
    "plt.bar(x, g, color='#7ed6df')\n",
    "plt.xlabel(\"Layers\")\n",
    "plt.ylabel(\"Processing Time\")\n",
    "plt.title(\"Processing Time of each Layer\")\n",
    "plt.xticks(x_pos, x,rotation=90)\n",
    "print('Sum of all the layers time(s):')\n",
    "print( sum(g))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from net_flops import net_flops\n",
    "# from time_per_layer import time_per_layer\n",
    "net_flops(model,table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #model.fit()\n",
    "# history=model.fit(datagen.flow(x_train, y_train2,batch_size=batch_size),\n",
    "#           epochs=20,\n",
    "#           verbose=1,\n",
    "#           validation_data=(x_test, y_test2))\n",
    "\n",
    "history=model.fit(x_train, y_train2,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test2))\n",
    "\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=50, verbose=2)\n",
    "# # 训练\n",
    "# history = model.fit(x_train, y_train2, epochs=20, batch_size=128, validation_data=(x_test, y_test2), verbose=2, shuffle=False, callbacks=[early_stopping])\n",
    "#monitor: 需要监视的量，val_loss or val_acc\n",
    "#patience: 当early stop被激活(如发现loss相比上一个epoch训练没有下降)，则经过patience个epoch后停止训练\n",
    "#verbose: 信息展示模式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation \n",
    "\n",
    "Evaluate your model with test dataset.\n",
    "\n",
    "make sure your model accuracy should be more than 99 percent. Otherwise, you have to go back to fine tune your model with whatever methods you have learned to improve its accuracy. You have to handle over-fitting or under-fitting problem you may encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#score = model.evaluate(xxxxxxx)\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])\n",
    "score = model.evaluate(x_test, y_test2, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your model weights\n",
    "\n",
    "You should save your model for car plate recognition purpose later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('weights_train_model1129alexnet1.h5')\n",
    "model.save('train_model1129alexnet1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(8, 8))\n",
    "#plt.subplot(1, 2, 1)\n",
    "#plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "#plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "#plt.legend(loc='lower right')\n",
    "#plt.title('Training and Validation Accuracy')\n",
    "\n",
    "#plt.subplot(1, 2, 2)\n",
    "#plt.plot(epochs_range, loss, label='Training Loss')\n",
    "#plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "#plt.legend(loc='upper right')\n",
    "#plt.title('Training and Validation Loss')\n",
    "#plt.show()\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(1, len(loss) + 1)\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.savefig(\"1129Alexnet1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
